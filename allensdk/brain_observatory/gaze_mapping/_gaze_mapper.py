import numpy as np
import pandas as pd

from scipy.spatial.transform import Rotation


class EyeTrackingRigObject(object):
    """Class encompassing coordinate transforms on objects in the
    eye tracking rig (camera, monitor).

    Parameters
    ----------
    position_in_eye_coord_frame : numpy.ndarray
        [x, y, z] position of the rig object in the eye coordinate system
    rotations_in_self_coord_frame: numpy.ndarray
        [x, y, z] rotations about the x, then y, then z axes to be applied
        in the rig object's own coordinate system

    """
    def __init__(self,
                 position_in_eye_coord_frame: np.ndarray,
                 rotations_in_self_coord_frame: np.ndarray):
        self.position = position_in_eye_coord_frame
        self.rotations = rotations_in_self_coord_frame

    def generate_rotations_xform(self) -> Rotation:
        return generate_object_rotation_xform(*self.rotations)

    def compute_unit_normal_in_eye_coord_frame(self) -> np.ndarray:
        """Compute unit normal to the object XY plane in eye coordinates."""
        self_to_eye_frame_xform = self.generate_self_to_eye_frame_xform()
        return self_to_eye_frame_xform.apply(self._compute_unit_normal())

    def _compute_unit_normal(self) -> np.ndarray:
        """Compute the unit normal vector for the object
        (after its orientation rotations have been applied).
        """
        # By convention Z-axis is the normal axis for both camera and monitor
        # rig imaging/screen planes
        unit_normal = [0, 0, 1]

        rotation_xform = self.generate_rotations_xform()
        return rotation_xform.apply(unit_normal)

    def generate_self_to_eye_frame_xform(self) -> Rotation:
        """Generate rotation matrix to transform base object coordinate frame
        to eye coordinate frame.

        By convention, any other object's coordinate frame before rotations
        is set with positive Z pointing from the object's position back
        to the origin of the eye coordinate system, with X parallel to the
        eye X-Y plane.

        Returns
        -------
        scipy.spatial.transform.Rotation
            A Rotation instance which will transform from an object's
            coordinate system (CCS/MCS) to the eye coordinate system (ECS)
        """
        # Determine unit normal vector representing +Z axis of CCS/MCS
        # in terms of ECS
        obj_norm = -(self.position / np.linalg.norm(self.position))

        # Determine rotation in ECS needed to rotate obj_norm vector so that
        # its x-axis aligns with the ECS x-axis.
        theta_z = -(np.pi / 2 + np.arctan2(obj_norm[1], obj_norm[0]))
        rz = Rotation.from_euler('z', theta_z, degrees=False)
        obj_norm_prime = rz.apply(obj_norm)

        # Determine rotation in ECS needed to rotate transformed obj_norm
        # vector so that its z-axis aligns with the ECS z-axis
        theta_x = np.pi / 2 - np.arctan2(obj_norm_prime[2], obj_norm_prime[1])
        rx = Rotation.from_euler('x', theta_x, degrees=False)

        # Compose rotations, note the order!
        eye_to_object_xform = rx * rz
        return eye_to_object_xform.inv()


class GazeMapper(object):
    """Class for performing eye-tracking gaze mapping.

    Provides methods for estimating the position of the pupil in
    3D space and map the gaze onto the monitor in both
    3D space and monitor space given the experimental geometry.

    Parameters
    ----------
    monitor_position : numpy.ndarray
        [x,y,z] position of monitor in cm.
    monitor_rotations : numpy.ndarray
        [x,y,z] rotations of monitor in radians.
    led_position : numpy.ndarray
        [x,y,z] position of LED in cm.
    camera_position : numpy.ndarray
        [x,y,z] position of camera in cm.
    camera_rotations : numpy.ndarray
        [x,y,z] rotations for camera in radians. X and Y must be 0.
    eye_radius : float
        Radius of the eye in cm.
    cm_per_pixel : float
        Pixel size of eye-tracking camera.
    """
    def __init__(self,
                 monitor_position: np.ndarray,
                 monitor_rotations: np.ndarray,
                 led_position: np.ndarray,
                 camera_position: np.ndarray,
                 camera_rotations: np.ndarray,
                 eye_radius: float,
                 cm_per_pixel: float):
        self.eye_radius = eye_radius
        self.cm_per_pixel = cm_per_pixel
        self.led_pos = led_position
        self.monitor = EyeTrackingRigObject(position_in_eye_coord_frame=monitor_position,
                                            rotations_in_self_coord_frame=monitor_rotations)
        self.camera = EyeTrackingRigObject(position_in_eye_coord_frame=camera_position,
                                           rotations_in_self_coord_frame=camera_rotations)
        self.cr = self.compute_cr_coordinate()

    def compute_cr_coordinate(self) -> np.ndarray:
        """Determine the 3D position of the corneal reflection (cr).

        Model the eye as a spherical mirror, so use the mirror
        equation to determine where the virtual image of the led would
        appear to be coming from if looking at the eye (like the camera is).

        Definitions:
        - focal length:
            radius_of_curvature/2
        - mirror equations:
            1/focal_length = 1/object_distance + 1/image_distance
            magnification = image_height/object_height
                          = -image_distance/object_distance

        Conventions:
        - Center of right eye is considered origin (x=0, y=0, z=0)
        - To use mirror equation (object_distance, image_distance) variables
          need to be offset so that the mirror pole is considered origin
          (x=0, y=0, z=0).
        - Focal length is negative for convex mirrors
        - Objects in front of mirror have positive distance
        - Objects behind mirror (virtual image) have negative distance

        Returns
        -------
        numpy.ndarray
            [x,y,z] location of the corneal reflection in eye coordinates (cm).
        """
        focal_len = -(self.eye_radius / 2)
        # In system conventions, Z gives the 'height' of our LED (object)
        object_height = self.led_pos[-1]
        # Object distance from the mirror pole is the euclidean norm of our
        # x and y coordinate components minus the eye_radius.
        object_dist = np.linalg.norm(self.led_pos[:2]) - self.eye_radius
        # Alternate form of mirror equation
        image_dist = (object_dist * focal_len) / (object_dist - focal_len)
        # Undo mirror pole offset
        image_dist_from_origin = self.eye_radius + image_dist
        image_height = -(image_dist / object_dist) * object_height
        image_dist_from_origin_mag = np.linalg.norm([image_height,
                                                     image_dist_from_origin])
        # To get full 3D position of virtual image we multiply the LED unit
        # position vector with magnitude of the image distance from origin.
        led_unit_position_vec = (self.led_pos / np.linalg.norm(self.led_pos))
        return led_unit_position_vec * image_dist_from_origin_mag

    def pupil_pos_in_eye_coords(self,
                                cam_pupil_params: np.ndarray,
                                cam_cr_params: np.ndarray) -> np.ndarray:
        """Compute the 3D pupil position in eye coordinates.

        Parameters
        ----------
        cam_pupil_params :  numpy.ndarray
            [nx2] Array of pupil parameters (x, y) for each eye tracking frame.
        cam_cr_params : numpy.ndarray
            [nx2] Array of corneal reflection parameters (x, y) for each eye
            tracking frame.

        Returns
        -------
        numpy.ndarray
            Pupil position estimates in eye coordinates (in centimeters).
        """
        # x, y are in camera image coordinates
        # x increases towards the right of image
        # y increases towards the bottom of image
        pupil_cr_delta = (cam_pupil_params - cam_cr_params) * self.cm_per_pixel
        delta_px = pupil_cr_delta.T[0]
        delta_py = pupil_cr_delta.T[1]

        R_eye_to_cam = self.camera.generate_self_to_eye_frame_xform().inv()
        R_cam = self.camera.generate_rotations_xform()

        cr_pos_in_cam_coord_frame = R_cam.apply(R_eye_to_cam.apply(self.cr))
        px_cam = cr_pos_in_cam_coord_frame[0] + delta_px
        py_cam = cr_pos_in_cam_coord_frame[1] + delta_py
        # np.sqrt(np.array([-5, 25])) will result in np.array([np.nan,  5.])
        # and an 'invalid' value RuntimeWarning which is fine
        with np.errstate(invalid='ignore'):
            pz_cam = np.sqrt(self.eye_radius**2 - px_cam**2 - py_cam**2)

        # Find and assign np.nan to pupil positions which land outside of eyeball radius.
        # An operation like: np.array([np.nan, 5, 1]) > 2 will result in array([False, True, False])
        # and an 'invalid' value RuntimeWarning which is fine
        with np.errstate(invalid='ignore'):
            bad_idx = np.linalg.norm([px_cam, py_cam], axis=0) > self.eye_radius
        px_cam[bad_idx] = np.nan
        py_cam[bad_idx] = np.nan
        pz_cam[bad_idx] = np.nan

        # Create [nx3] pupil position (x, y, z) estimates
        pupil_pos_cam = np.vstack([px_cam, py_cam, pz_cam]).T

        # Undo 'cam rotation' and 'eye to cam rotation' to get
        # pupil positions in eye coordinates (in centimeters)
        cam_to_eye_xform = R_eye_to_cam.inv() * R_cam.inv()
        return cam_to_eye_xform.apply(pupil_pos_cam)

    def pupil_position_on_monitor_in_cm(self,
                                        cam_pupil_params: np.ndarray,
                                        cam_cr_params: np.ndarray) -> np.ndarray:
        """Compute the pupil position on the monitor in cm.

        General strategy:
        1) Figure out the positions of pupil center in eye coordinates
           Using pre-calculated (compute_cr_coordinate) corneal reflection
           virtual image location as a reference point.

        2) Project a ray from origin through an estimated pupil position
           and determine the point (in eye coordinate system) at which it
           intersects a plane representing the monitor

        3) Convert the intersection point into the monitor coordinate system

        Parameters
        ----------
        cam_pupil_params :  numpy.ndarray
            [nx2] Array of pupil parameters (x, y) for each eye tracking frame.
        cam_cr_params : numpy.ndarray
            [nx2] Array of corneal reflection parameters (x, y) for each eye
            tracking frame.

        Returns
        -------
        numpy.ndarray
            [nx2] Pupil position estimates (x, y) for each frame in eye
            coordinates (in centimeters). Estimate values will have the
            center of the monitor as the (0, 0) origin.
        """
        pupil_positions = self.pupil_pos_in_eye_coords(cam_pupil_params,
                                                       cam_cr_params)

        monitor_normal = self.monitor.compute_unit_normal_in_eye_coord_frame()
        # Project pupil locations from origin of eye coordinate system
        line_points = np.tile([0, 0, 0], (pupil_positions.shape[0], 1))
        projected_positions = project_to_plane(plane_normal=monitor_normal,
                                               plane_point=self.monitor.position,
                                               line_vectors=pupil_positions,
                                               line_points=line_points)

        monitor_positions = projected_positions - self.monitor.position

        R_monitor = self.monitor.generate_rotations_xform()
        R_monitor_to_eye = self.monitor.generate_self_to_eye_frame_xform()
        eye_to_monitor_xform = R_monitor.inv() * R_monitor_to_eye.inv()
        result = eye_to_monitor_xform.apply(monitor_positions)

        # Discard z component of monitor locs as it's orthogonal to viewing plane
        return np.delete(result, 2, axis=1)

    def pupil_position_on_monitor_in_degrees(self,
                                             pupil_pos_on_monitor_in_cm: np.ndarray) -> np.ndarray:
        """Get pupil position on monitor measured in visual degrees.

        Parameters
        ----------
        pupil_pos_on_monitor_in_cm :  numpy.ndarray
            [nx2] Array of pupil positions mapped to monitor coordinates (x, y)

        Returns
        -------
        numpy.ndarray
            [nx2] Pupil position estimate (x, y) in visual degrees.
        """
        x = pupil_pos_on_monitor_in_cm.T[0]
        y = pupil_pos_on_monitor_in_cm.T[1]

        mag = np.linalg.norm(self.monitor.position)
        meridian = np.degrees(np.arctan(x / mag))
        elevation = np.degrees(np.arctan(y / np.linalg.norm([x, mag], axis=0)))

        angles = np.vstack([meridian, elevation]).T

        return angles


def compute_circular_areas(ellipse_params: pd.DataFrame) -> pd.Series:
    """Compute circular area of a pupil using half-major axis.

    Assume the pupil is a circle, and that as it moves off-axis
    with the camera, the observed ellipse semi-major axis remains the
    radius of the circle.

    Parameters
    ----------
    ellipse_params (pandas.DataFrame): A table of pupil parameters consisting
        of 5 columns: ("center_x", "center_y", "height", "phi", "width")
        and n-row timepoints.

        NOTE: For ellipse_params produced by the Deep Lab Cut pipeline,
        "width" and "height" columns, in fact, refer to the
        "half-width" and "half-height".

    Returns
    -------
        pandas.Series: A series of pupil areas for n-timepoints.
    """
    # Take the biggest value between height and width columns and
    # assume that it is the pupil circle radius.
    radii = ellipse_params[["height", "width"]].max(axis=1)
    return np.pi * radii * radii


def compute_elliptical_areas(ellipse_params: pd.DataFrame) -> pd.Series:
    """Compute the elliptical area using elliptical fit parameters.

    Parameters
    ----------
    ellipse_params (pandas.DataFrame): A table of pupil parameters consisting
        of 5 columns: ("center_x", "center_y", "height", "phi", "width")
        and n-row timepoints.

        NOTE: For ellipse_params produced by the Deep Lab Cut pipeline,
        "width" and "height" columns, in fact, refer to the
        "half-width" and "half-height".

    Returns
    -------
    pd.Series
        pandas.Series: A series of areas for n-timepoints.
    """
    return np.pi * ellipse_params["height"] * ellipse_params["width"]


def project_to_plane(plane_normal: np.ndarray,
                     plane_point: np.ndarray,
                     line_vectors: np.ndarray,
                     line_points: np.ndarray) -> np.ndarray:
    """Find the points of intersection between a plane and a series of lines.

    See: https://en.wikipedia.org/wiki/Lineâ€“plane_intersection

    Parameters
    ----------
    plane_normal : numpy.ndarray
        [x, y, z] normal vector for the plane.
    plane_point : numpy.ndarray
        [x, y, z] A point on the plane.
    line_vectors : numpy.ndarray
        [nx3] A sequence of 'n' vectors (x, y, z) each representing a line.
    line_points : numpy.ndarray
        [nx3] A sequence of 'n' (x, y, z) values which specify a point on the
        corresponding 'n'th line vector.

    Returns
    -------
    numpy.ndarray
        [nx3] A sequence of 'n' (x, y, z) coordinates which represent the
        point of intersection between the plane and the 'n'th line vector.
    """
    factors = np.dot((plane_point - line_points), plane_normal) / np.dot(line_vectors, plane_normal)
    factors = factors.reshape(-1, 1)

    return factors * line_vectors + line_points


def generate_object_rotation_xform(x_rotation: float,
                                   y_rotation: float,
                                   z_rotation: float) -> Rotation:
    """Generate a matrix for rotating an object in place.

    Parameters
    ----------
    x_rotation : float
        Rotation about x axis in radians.
    y_rotation : float
        Rotation about y' axis in radians.
    z_rotation : float
        Rotation about z'' axis in radians.

    -------
    Rotation (scipy.spatial.transform.Rotation)
        A rotation instance. See:
        https://docs.scipy.org/doc/scipy/reference/generated/scipy.spatial.transform.Rotation.html
    """
    rx = Rotation.from_euler('x', x_rotation, degrees=False)
    ry = Rotation.from_euler('y', y_rotation, degrees=False)
    rz = Rotation.from_euler('z', z_rotation, degrees=False)

    # Compose rotations with * operator. Note the order!
    return rz * ry * rx
