import logging
from typing import List, Optional

import numpy as np
from pynwb import NWBFile

from allensdk.brain_observatory.ecephys._channels import Channels
from allensdk.brain_observatory.ecephys._unit import Unit
from allensdk.brain_observatory.ecephys.utils import load_and_squeeze_npy, \
    scale_amplitudes, group_1d_by_unit
from allensdk.core import DataObject, NwbReadableInterface, \
    JsonReadableInterface


class Units(DataObject, JsonReadableInterface, NwbReadableInterface):
    """
    A collection of units
    """

    def __init__(self, units: List[Unit]):
        super().__init__(name='units', value=units)

    @classmethod
    def from_json(
            cls,
            probe: dict,
            amplitude_scale_factor=0.195e-6
    ) -> "Units":
        """

        Parameters
        ----------
        probe
        amplitude_scale_factor: amplitude scale factor converting raw
        amplitudes to Volts. Default converts from bits -> uV -> V

        Returns
        -------

        """
        local_to_global_unit_map = {
            unit['cluster_id']: unit['id'] for unit in probe['units']}
        spike_times = _read_spike_times_to_dictionary(
            probe['spike_times_path'],
            probe['spike_clusters_file'],
            local_to_global_unit_map
        )
        mean_waveforms = _read_waveforms_to_dictionary(
            probe['mean_waveforms_path'],
            local_to_global_unit_map
        )
        spike_amplitudes = _read_spike_amplitudes_to_dictionary(
            probe["spike_amplitudes_path"],
            probe["spike_clusters_file"],
            probe["templates_path"],
            probe["spike_templates_path"],
            probe["inverse_whitening_matrix_path"],
            local_to_global_unit_map=local_to_global_unit_map,
            scale_factor=probe.get('amplitude_scale_factor',
                                   amplitude_scale_factor)
        )
        units = [
            Unit(**unit,
                 spike_times=spike_times[unit['id']],
                 spike_amplitudes=spike_amplitudes[unit['id']],
                 mean_waveforms=mean_waveforms[unit['id']])
            for unit in probe['units']
        ]
        units = Units(units=units)
        return units

    @classmethod
    def from_nwb(cls, nwbfile: NWBFile,
                 probe_id: Optional[str] = None) -> "Units":
        """

        Parameters
        ----------
        nwbfile
        probe_id: Filter units to just those detected by this `probe_id`

        Returns
        -------
        `Units`
        """
        units = nwbfile.units.to_dataframe()
        units = units.reset_index()
        units = units.rename(columns={'waveform_mean': 'mean_waveforms'})

        if probe_id is not None:
            channels = Channels.from_nwb(nwbfile=nwbfile)
            units = units[units['peak_channel_id'].map(
                {c.id: c.probe_id for c in channels.value}) == probe_id]

        units = units.to_dict(orient='records')

        units = [Unit(**unit, filter_and_sort_spikes=False) for unit in units]
        return Units(units=units)


def _read_spike_amplitudes_to_dictionary(
        spike_amplitudes_path, spike_units_path,
        templates_path, spike_templates_path, inverse_whitening_matrix_path,
        local_to_global_unit_map=None,
        scale_factor=0.195e-6
):
    spike_amplitudes = load_and_squeeze_npy(spike_amplitudes_path)
    spike_units = load_and_squeeze_npy(spike_units_path)

    templates = load_and_squeeze_npy(templates_path)
    spike_templates = load_and_squeeze_npy(spike_templates_path)
    inverse_whitening_matrix = \
        load_and_squeeze_npy(inverse_whitening_matrix_path)

    for temp_idx in range(templates.shape[0]):
        templates[temp_idx, :, :] = np.dot(
            np.ascontiguousarray(templates[temp_idx, :, :]),
            np.ascontiguousarray(inverse_whitening_matrix)
        )

    scaled_amplitudes = scale_amplitudes(spike_amplitudes,
                                         templates,
                                         spike_templates,
                                         scale_factor=scale_factor)

    return group_1d_by_unit(scaled_amplitudes,
                            spike_units,
                            local_to_global_unit_map)


def _read_waveforms_to_dictionary(
        waveforms_path, local_to_global_unit_map=None, peak_channel_map=None
):
    """ Builds a lookup table for unitwise waveform data

    Parameters
    ----------
    waveforms_path : str
        npy file containing waveform data for each unit. Dimensions ought to
        be units X samples X channels
    local_to_global_unit_map : dict, optional
        Maps probewise local unit indices to global unit ids
    peak_channel_map : dict, optional
        Maps unit identifiers to indices of peak channels. If provided,
        the output will contain only samples on the peak
        channel for each unit.

    Returns
    -------
    output_waveforms : dict
        Keys are unit identifiers, values are samples X channels data arrays.

    """

    waveforms = np.squeeze(np.load(waveforms_path, allow_pickle=False))
    output_waveforms = {}
    for unit_id, waveform in enumerate(
            np.split(waveforms, waveforms.shape[0], axis=0)
    ):
        if local_to_global_unit_map is not None:
            if unit_id not in local_to_global_unit_map:
                logging.warning(
                    f"""unable to find unit at local position
                        {unit_id} while reading waveforms"""
                )
                continue
            unit_id = local_to_global_unit_map[unit_id]

        if peak_channel_map is not None:
            waveform = waveform[:, peak_channel_map[unit_id]]

        output_waveforms[unit_id] = np.squeeze(waveform)

    return output_waveforms


def _read_spike_times_to_dictionary(
        spike_times_path, spike_units_path, local_to_global_unit_map=None
):
    """ Reads spike times and assigned units from npy files into a lookup
    table.

    Parameters
    ----------
    spike_times_path : str
        npy file identifying, per spike, the time at which that spike occurred.
    spike_units_path : str
        npy file identifying, per spike, the unit associated with that spike.
        These are probe-local, so a local_to_global_unit_map is used to
        associate spikes with global unit identifiers.
    local_to_global_unit_map : dict, optional
        Maps probewise local unit indices to global unit ids

    Returns
    -------
    output_times : dict
        keys are unit identifiers, values are spike time arrays

    """

    spike_times = load_and_squeeze_npy(spike_times_path)
    spike_units = load_and_squeeze_npy(spike_units_path)

    return group_1d_by_unit(spike_times, spike_units, local_to_global_unit_map)
