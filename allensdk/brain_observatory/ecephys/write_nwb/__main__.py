"""Module for writing NWB files for the VCN project"""
import logging
import sys
from typing import Any, Dict, List, Tuple
from pathlib import Path, PurePath
import multiprocessing as mp
from functools import partial

import pynwb
import requests
import pandas as pd
import numpy as np

from allensdk.brain_observatory.behavior.data_objects.stimuli.presentations \
    import \
    Presentations
from allensdk.brain_observatory.ecephys._behavior_ecephys_metadata import \
    BehaviorEcephysMetadata
from allensdk.brain_observatory.ecephys._probe import Probe
from allensdk.brain_observatory.ecephys.optotagging import OptotaggingTable
from allensdk.brain_observatory.ecephys.probes import Probes
from allensdk.brain_observatory.ecephys.write_nwb.schemas import \
    VCNInputSchema, OutputSchema
from allensdk.config.manifest import Manifest

from allensdk.brain_observatory.nwb import (
    add_stimulus_timestamps,
    add_invalid_times,
    read_eye_dlc_tracking_ellipses,
    read_eye_gaze_mappings,
    add_eye_tracking_ellipse_fit_data_to_nwbfile,
    add_eye_gaze_mapping_data_to_nwbfile,
    eye_tracking_data_is_valid
)
from allensdk.brain_observatory.argschema_utilities import (
    optional_lims_inputs
)

from allensdk.brain_observatory.ecephys.nwb import (
    EcephysSpecimen,
    EcephysEyeTrackingRigMetadata)
from allensdk.brain_observatory.sync_dataset import Dataset
import allensdk.brain_observatory.sync_utilities as su


STIM_TABLE_RENAMES_MAP = {"Start": "start_time", "End": "stop_time"}


def get_inputs_from_lims(host,
                         ecephys_session_id,
                         output_root,
                         job_queue,
                         strategy):
    """
     This is a development / testing utility for running this module from the
     Allen Institute for Brain Science's Laboratory Information Management
     System (LIMS). It will only work if you are on our internal network.

    Parameters
    ----------
    ecephys_session_id : int
        Unique identifier for session of interest.
    output_root : str
        Output file will be written into this directory.
    job_queue : str
        Identifies the job queue from which to obtain configuration data
    strategy : str
        Identifies the LIMS strategy which will be used to write module inputs.

    Returns
    -------
    data : dict
        Response from LIMS. Should meet the schema defined in _schemas.py

    """

    uri = f"{host}/input_jsons?object_id={ecephys_session_id}" + \
          f"&object_class=EcephysSession&strategy_class={strategy}" + \
          f"&job_queue_name={job_queue}&output_directory={output_root}"

    response = requests.get(uri)
    data = response.json()

    if len(data) == 1 and "error" in data:
        raise ValueError("bad request uri: {} ({})".format(uri, data["error"]))

    return data


def read_stimulus_table(path: str,
                        column_renames_map: Dict[str, str] = None,
                        columns_to_drop: List[str] = None) -> pd.DataFrame:
    """ Loads from a CSV on disk the stimulus table for this session.
    Optionally renames columns to match NWB epoch specifications.

    Parameters
    ----------
    path : str
        path to stimulus table csv
    column_renames_map : Dict[str, str], optional
        If provided, will be used to rename columns from keys -> values.
        Default renames: ('Start' -> 'start_time') and ('End' -> 'stop_time')
    columns_to_drop : List, optional
        A list of column names to drop. Columns will be dropped BEFORE
        any renaming occurs. If None, no columns are dropped.
        By default None.

    Returns
    -------
    pd.DataFrame :
        stimulus table with applied renames

    """
    if column_renames_map is None:
        column_renames_map = STIM_TABLE_RENAMES_MAP

    ext = PurePath(path).suffix

    if ext == ".csv":
        stimulus_table = pd.read_csv(path)
    else:
        raise IOError(f"unrecognized stimulus table extension: {ext}")

    if columns_to_drop:
        stimulus_table = stimulus_table.drop(errors='ignore',
                                             columns=columns_to_drop)

    return stimulus_table.rename(columns=column_renames_map, index={})


def add_metadata_to_nwbfile(nwbfile, input_metadata):
    metadata = input_metadata.copy()

    if "full_genotype" in metadata:
        metadata["genotype"] = metadata.pop("full_genotype")

    if "stimulus_name" in metadata:
        nwbfile.stimulus_notes = metadata.pop("stimulus_name")

    if "age_in_days" in metadata:
        metadata["age"] = f"P{int(metadata['age_in_days'])}D"

    if "donor_id" in metadata:
        metadata["subject_id"] = str(metadata.pop("donor_id"))

    nwbfile.subject = EcephysSpecimen(**metadata)
    return nwbfile


def read_running_speed(path):
    """ Reads running speed data and timestamps into a RunningSpeed named tuple

    Parameters
    ----------
    path : str
        path to running speed store


    Returns
    -------
    tuple :
        first item is dataframe of running speed data, second is dataframe of
        raw values (vsig, vin, encoder rotation)

    """

    return (
        pd.read_hdf(path, key="running_speed"),
        pd.read_hdf(path, key="raw_data")
    )


DEFAULT_RUNNING_SPEED_UNITS = {
    "velocity": "cm/s",
    "vin": "V",
    "vsig": "V",
    "rotation": "radians"
}


def add_running_speed_to_nwbfile(nwbfile, running_speed, units=None):
    if units is None:
        units = DEFAULT_RUNNING_SPEED_UNITS

    running_mod = pynwb.ProcessingModule("running", "running speed data")
    nwbfile.add_processing_module(running_mod)

    running_speed_timeseries = pynwb.base.TimeSeries(
        name="running_speed",
        timestamps=running_speed["start_time"].values,
        data=running_speed["velocity"].values,
        unit=units["velocity"]
    )

    # Create an 'empty' timeseries that only stores end times
    # An array of nans needs to be created to avoid an nwb schema violation
    running_speed_end_timeseries = pynwb.base.TimeSeries(
        name="running_speed_end_times",
        data=np.full(running_speed["velocity"].shape, np.nan),
        timestamps=running_speed["end_time"].values,
        unit=units["velocity"]
    )

    rotation_timeseries = pynwb.base.TimeSeries(
        name="running_wheel_rotation",
        timestamps=running_speed_timeseries,
        data=running_speed["net_rotation"].values,
        unit=units["rotation"]
    )

    running_mod.add_data_interface(running_speed_timeseries)
    running_mod.add_data_interface(running_speed_end_timeseries)
    running_mod.add_data_interface(rotation_timeseries)

    return nwbfile


def add_raw_running_data_to_nwbfile(nwbfile, raw_running_data, units=None):
    if units is None:
        units = DEFAULT_RUNNING_SPEED_UNITS

    raw_rotation_timeseries = pynwb.base.TimeSeries(
        name="raw_running_wheel_rotation",
        timestamps=np.array(raw_running_data["frame_time"]),
        data=raw_running_data["dx"].values,
        unit=units["rotation"]
    )

    vsig_ts = pynwb.base.TimeSeries(
        name="running_wheel_signal_voltage",
        timestamps=raw_rotation_timeseries,
        data=raw_running_data["vsig"].values,
        unit=units["vsig"]
    )

    vin_ts = pynwb.base.TimeSeries(
        name="running_wheel_supply_voltage",
        timestamps=raw_rotation_timeseries,
        data=raw_running_data["vin"].values,
        unit=units["vin"]
    )

    nwbfile.add_acquisition(raw_rotation_timeseries)
    nwbfile.add_acquisition(vsig_ts)
    nwbfile.add_acquisition(vin_ts)

    return nwbfile


def write_probe_lfp_file(session_id, session_metadata, session_start_time,
                         log_level, probe_meta):
    """ Writes LFP data (and associated channel information) for one
    probe to a standalone nwb file
    """
    logging.getLogger('').setLevel(log_level)
    logging.info(f"writing lfp file for probe {probe_meta['id']}")

    probe = Probe.from_json(probe=probe_meta)
    nwbfile = probe.add_lfp_to_nwb(
        session_id=session_id,
        session_start_time=session_start_time,
        session_metadata=BehaviorEcephysMetadata.from_json(
            dict_repr=session_metadata)
    )
    with pynwb.NWBHDF5IO(probe_meta['lfp']['output_path'], 'w') as lfp_writer:
        logging.info(f"writing lfp file to {probe_meta['lfp']['output_path']}")
        lfp_writer.write(nwbfile, cache_spec=True)
    return {
        "id": probe_meta["id"],
        "nwb_path": probe_meta["lfp"]["output_path"]}


def write_probewise_lfp_files(probes, session_id, session_metadata,
                              session_start_time, pool_size=3):

    output_paths = []

    pool = mp.Pool(processes=pool_size)
    write = partial(write_probe_lfp_file,
                    session_id,
                    session_metadata,
                    session_start_time,
                    logging.getLogger("").getEffectiveLevel())

    for pout in pool.imap_unordered(write, probes):
        output_paths.append(pout)

    return output_paths


ParsedProbeData = Tuple[pd.DataFrame,  # unit_tables
                        Dict[int, np.ndarray],  # spike_times
                        Dict[int, np.ndarray],  # spike_amplitudes
                        Dict[int, np.ndarray]]  # mean_waveforms


def parse_probes_data(probes: List[Dict[str, Any]]) -> ParsedProbeData:
    """Given a list of probe dictionaries specifying data file locations, load
    and parse probe data into intermediate data structures needed for adding
    probe data to an nwbfile.

    Parameters
    ----------
    probes : List[Dict[str, Any]]
        A list of dictionaries (one entry for each probe), where each probe
        dictionary contains metadata (id, name, sampling_rate, etc...) as well
        as filepaths pointing to where probe lfp data can be found.

    Returns
    -------
    ParsedProbeData : Tuple[...]
        unit_tables : pd.DataFrame
            A table containing unit metadata from all probes.
        spike_times : Dict[int, np.ndarray]
            Keys: unit identifiers, Values: spike time arrays
        spike_amplitudes : Dict[int, np.ndarray]
            Keys: unit identifiers, Values: spike amplitude arrays
        mean_waveforms : Dict[int, np.ndarray]
            Keys: unit identifiers, Values: mean waveform arrays
    """
    probes = Probes.from_json(probes=probes)
    return probes.units_table, \
        probes.spike_times, \
        probes.spike_amplitudes, \
        probes.mean_waveforms


def add_probewise_data_to_nwbfile(nwbfile, probes):
    """ Adds channel (electrode) and spike data for a single probe to
        the session-level nwb file.
    """
    probes = Probes.from_json(probes=probes)
    probes.to_nwb(nwbfile=nwbfile)

    return nwbfile


def add_eye_tracking_rig_geometry_data_to_nwbfile(
        nwbfile: pynwb.NWBFile,
        eye_tracking_rig_geometry: dict) -> pynwb.NWBFile:

    """ Rig geometry dict should consist of the following fields:
    monitor_position_mm: [x, y, z]
    monitor_rotation_deg: [x, y, z]
    camera_position_mm: [x, y, z]
    camera_rotation_deg: [x, y, z]
    led_position: [x, y, z]
    equipment: A string describing rig
    """
    eye_tracking_rig_mod = \
        pynwb.ProcessingModule(name='eye_tracking_rig_metadata',
                               description='Eye tracking rig metadata module')

    rig_metadata = EcephysEyeTrackingRigMetadata(
        name="eye_tracking_rig_metadata",
        equipment=eye_tracking_rig_geometry['equipment'],
        monitor_position=eye_tracking_rig_geometry['monitor_position_mm'],
        monitor_position__unit="mm",
        camera_position=eye_tracking_rig_geometry['camera_position_mm'],
        camera_position__unit="mm",
        led_position=eye_tracking_rig_geometry['led_position'],
        led_position__unit="mm",
        monitor_rotation=eye_tracking_rig_geometry['monitor_rotation_deg'],
        monitor_rotation__unit="deg",
        camera_rotation=eye_tracking_rig_geometry['camera_rotation_deg'],
        camera_rotation__unit="deg"
    )

    eye_tracking_rig_mod.add_data_interface(rig_metadata)
    nwbfile.add_processing_module(eye_tracking_rig_mod)

    return nwbfile


def add_eye_tracking_data_to_nwbfile(
        nwbfile: pynwb.NWBFile,
        eye_tracking_frame_times: pd.Series,
        eye_dlc_tracking_data: Dict[str, pd.DataFrame],
        eye_gaze_data: Dict[str, pd.DataFrame]) -> pynwb.NWBFile:

    if eye_tracking_data_is_valid(eye_dlc_tracking_data=eye_dlc_tracking_data,
                                  synced_timestamps=eye_tracking_frame_times):

        add_eye_tracking_ellipse_fit_data_to_nwbfile(
            nwbfile,
            eye_dlc_tracking_data=eye_dlc_tracking_data,
            synced_timestamps=eye_tracking_frame_times)

        # --- Add gaze mapped positions to nwb file ---
        if eye_gaze_data:
            add_eye_gaze_mapping_data_to_nwbfile(nwbfile,
                                                 eye_gaze_data=eye_gaze_data)

    return nwbfile


def write_ecephys_nwb(
    output_path,
    session_id, session_start_time,
    stimulus_table_path,
    invalid_epochs,
    probes,
    running_speed_path,
    session_sync_path,
    pool_size,
    optotagging_table_path=None,
    eye_tracking_rig_geometry=None,
    eye_dlc_ellipses_path=None,
    eye_gaze_mapping_path=None,
    session_metadata=None,
    **kwargs
):

    nwbfile = pynwb.NWBFile(
        session_description='Data and metadata for an Ecephys session',
        identifier=f"{session_id}",
        session_id=f"{session_id}",
        session_start_time=session_start_time,
        institution="Allen Institute"
    )

    if session_metadata is not None:
        nwbfile = add_metadata_to_nwbfile(nwbfile, session_metadata)

    stimulus_columns_to_drop = [
        "colorSpace", "depth", "interpolate", "pos", "rgbPedestal", "tex",
        "texRes", "flipHoriz", "flipVert", "rgb", "signalDots"
    ]
    stimulus_table = Presentations.from_path(
        path=stimulus_table_path,
        behavior_session_id=session_id,
        exclude_columns=stimulus_columns_to_drop,
        columns_to_rename=STIM_TABLE_RENAMES_MAP,
        sort_columns=False
    )
    nwbfile = \
        add_stimulus_timestamps(nwbfile,
                                stimulus_table.value['start_time'].values)
    nwbfile = stimulus_table.to_nwb(nwbfile=nwbfile)

    nwbfile = add_invalid_times(nwbfile, invalid_epochs)

    if optotagging_table_path is not None:
        optotagging_table = OptotaggingTable.from_json(
            dict_repr={'optotagging_table_path': optotagging_table_path})
        nwbfile = optotagging_table.to_nwb(nwbfile=nwbfile)

    nwbfile = add_probewise_data_to_nwbfile(nwbfile, probes)

    running_speed, raw_running_data = read_running_speed(running_speed_path)
    add_running_speed_to_nwbfile(nwbfile, running_speed)
    add_raw_running_data_to_nwbfile(nwbfile, raw_running_data)

    if eye_tracking_rig_geometry is not None:
        add_eye_tracking_rig_geometry_data_to_nwbfile(
            nwbfile,
            eye_tracking_rig_geometry
        )

    # Collect eye tracking/gaze mapping data from files
    if eye_dlc_ellipses_path is not None:
        eye_tracking_frame_times = \
            su.get_synchronized_frame_times(
                session_sync_file=session_sync_path,
                sync_line_label_keys=Dataset.EYE_TRACKING_KEYS
            )
        eye_dlc_tracking_data = \
            read_eye_dlc_tracking_ellipses(Path(eye_dlc_ellipses_path))

        if eye_gaze_mapping_path is not None:
            eye_gaze_data = read_eye_gaze_mappings(Path(eye_gaze_mapping_path))
        else:
            eye_gaze_data = None

        add_eye_tracking_data_to_nwbfile(nwbfile,
                                         eye_tracking_frame_times,
                                         eye_dlc_tracking_data,
                                         eye_gaze_data)

    Manifest.safe_make_parent_dirs(output_path)
    with pynwb.NWBHDF5IO(output_path, mode='w') as io:
        logging.info(f"writing session nwb file to {output_path}")
        io.write(nwbfile, cache_spec=True)

    probes_with_lfp = [p for p in probes if p["lfp"] is not None]
    probes_without_lfp = [p for p in probes if p["lfp"] is None]

    probe_outputs = write_probewise_lfp_files(probes_with_lfp, session_id,
                                              session_metadata,
                                              session_start_time,
                                              pool_size=pool_size)

    probe_outputs += \
        [{'id': p["id"], "nwb_path": ""} for p in probes_without_lfp]

    return {
        'nwb_path': output_path,
        "probe_outputs": probe_outputs
    }


def main():
    logging.basicConfig(
        format="%(asctime)s - %(process)s - %(levelname)s - %(message)s"
    )

    parser = optional_lims_inputs(
        sys.argv,
        VCNInputSchema,
        OutputSchema,
        get_inputs_from_lims
    )

    write_ecephys_nwb(**parser.args)

    # output = write_ecephys_nwb(**parser.args)

    # write_or_print_outputs(output, parser)


if __name__ == "__main__":
    main()
