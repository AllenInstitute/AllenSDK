import io, json
from os.path import join, basename
from glob import glob
import numpy as np
import os
import pwd
import pandas as pd
import datetime
import re

def createInputJson(directory, resort_directory, module, output_file, last_unit_id):

    session_id = basename(directory)

    userID = 'joshs'
    description = 'Neuropixels experiment in visual cortex'

    sync_file = glob(join(directory, '*.sync'))[0]

    LIMS_session_id = os.path.basename(sync_file).split('.')[0][:9]

    nwb_output_path = join('/mnt/nvme0/ecephys_nwb_files_20190827-2', session_id + '.spikes.nwb2')

    stimulus_table_path = join(directory, 'stim_table.csv')

    #print(resort_directory)

    probe_directories = glob(join(resort_directory, '*probe*','*probe*'))

    #print(probe_directories)

    probe_directories.sort()

    probes = []

    for probe_idx, probe_directory in enumerate(probe_directories):

        print(probe_directory)

        original_probe_directory = os.path.join(directory, os.path.basename(probe_directory))

        name = probe_directory[-13:-7]

        new_sorting_directory = glob(join(probe_directory, 'continuous', 'Neuropix-*-100.0'))[0]
        original_sorting_directory = glob(join(original_probe_directory, 'continuous', 'Neuropix-*-100.0'))[0]

        if original_sorting_directory.find('PXI') > -1:
            probe_type = 'PXI'
        else:
            probe_type = '3a'


        #lfp_dict = {
        #    'input_data_path' : 'none',
        #    'input_timestamps_path' : 'none',
        #    'input_channels_path' : 'none',
        #    'output_path' : 'none'
        #}
        timestamp_files = []

        timestamp_files.append(
            {   'name' : 'spike_timestamps',
                'input_path' : join(new_sorting_directory,'spike_times.npy'),
                'output_path' : join(new_sorting_directory, 'spike_times_master_clock.npy'),
            }
        )

        #timestamp_files.append(
        #    {   'name' : 'lfp_timestamps',
        #        'input_path' : join(new_sorting_directory,'spike_times.npy'),
        #        'output_path' : join(new_sorting_directory, 'spike_times_master_clock.npy'),
        #    }
        #)


        if module == 'allensdk.brain_observatory.ecephys.align_timestamps':
            probe_dict = {
                'name' : name,
                'sampling_rate' : 30000.,
                'lfp_sampling_rate' : 2500.,
                'barcode_channel_states_path' : join(original_probe_directory, 'events', 'Neuropix-' + probe_type + '-100.0', 'TTL_1', 'channel_states.npy'),
                'barcode_timestamps_path' : join(original_probe_directory, 'events', 'Neuropix-' + probe_type + '-100.0', 'TTL_1', 'event_timestamps.npy'),
                'mappable_timestamp_files' : timestamp_files,
            }
        else:
            channel_info = pd.read_csv(join(original_sorting_directory, 'ccf_regions_new.csv'), index_col=0)

            channels = []

            for idx, row in channel_info.iterrows():

                structure_acronym = row['structure_acronym']
                numbers = re.findall(r'\d+', structure_acronym)
                    
                if (len(numbers) > 0 and name[:2] != 'CA'):
                    structure_acronym = structure_acronym.split(numbers[0])[0]
                    cortical_layer = '/'.join(numbers)
                else:
                    cortical_layer = 'none'

                channel_dict = {
                    'id' : idx + probe_idx * 1000,
                    'valid_data' : row['is_valid'],
                    'probe_id' : probe_idx,
                    'local_index' : idx,
                    'probe_vertical_position' : row['vertical_position'],
                    'probe_horizontal_position' : row['horizontal_position'],
                    'structure_id' : row['structure_id'],
                    'cortical_layer' : cortical_layer,
                    'structure_acronym' : structure_acronym,
                    'AP_coordinate' : row['A/P'],
                    'DV_coordinate' : row['D/V'],
                    'ML_coordinate' : row['M/L'],
                    'cortical_depth' : row['cortical_depth']
                }

                channels.append(channel_dict)

            unit_info = pd.read_csv(join(new_sorting_directory, 'metrics.csv.v2'), index_col=0)
            #unit_quality = pd.read_csv(join(new_sorting_directory, 'cluster_group.tsv'), index_col=0, sep='\t')
            #unit_quality = unit_quality.replace(to_replace='unsorted',value='good')

            units = []

            print(len(unit_info))

            for idx, row in unit_info.iterrows():

                    if row['quality'] == 'good':

                        unit_dict = {
                            'id' : last_unit_id,
                            'peak_channel_id' : row['peak_channel'] + probe_idx * 1000,
                            'local_index' : idx,
                            'cluster_id' : row['cluster_id'],
                            'quality' : row['quality'],
                            'firing_rate' : cleanUpNanAndInf(row['firing_rate']), 
                            'snr' : cleanUpNanAndInf(row['snr']),
                            'isi_violations' : cleanUpNanAndInf(row['isi_viol']),
                            'presence_ratio' : cleanUpNanAndInf(row['presence_ratio']),
                            'amplitude_cutoff' : cleanUpNanAndInf(row['amplitude_cutoff']),
                            'isolation_distance' : cleanUpNanAndInf(row['isolation_distance']),
                            'l_ratio' : cleanUpNanAndInf(row['l_ratio']),
                            'd_prime' : cleanUpNanAndInf(row['d_prime']),
                            'nn_hit_rate' : cleanUpNanAndInf(row['nn_hit_rate']),
                            'nn_miss_rate' : cleanUpNanAndInf(row['nn_miss_rate']),
                            'max_drift' : cleanUpNanAndInf(row['max_drift']),
                            'cumulative_drift' : cleanUpNanAndInf(row['cumulative_drift']),
                            'silhouette_score' : cleanUpNanAndInf(row['silhouette_score']),
                            'waveform_duration' : cleanUpNanAndInf(row['duration']),
                            'waveform_halfwidth' : cleanUpNanAndInf(row['halfwidth']),
                            'waveform_PT_ratio' : cleanUpNanAndInf(row['PT_ratio']),
                            'waveform_repolarization_slope' : cleanUpNanAndInf(row['repolarization_slope']),
                            'waveform_recovery_slope' : cleanUpNanAndInf(row['recovery_slope']),
                            'waveform_amplitude' : cleanUpNanAndInf(row['amplitude']),
                            'waveform_spread' : cleanUpNanAndInf(row['spread']),
                            'waveform_velocity_above' : cleanUpNanAndInf(row['velocity_above']),
                            'waveform_velocity_below' : cleanUpNanAndInf(row['velocity_below'])
                        }

                        #if channel_info.loc[row['peak_channel']]['structure_acronym'] == 'VISp5':
                        units.append(unit_dict)
                        last_unit_id += 1

            #print(len(unit_info))

            probe_dict = {
                'id' : probe_idx,
                'name' : name,
                'spike_times_path' : join(new_sorting_directory, 'spike_times_master_clock.npy'),
                'spike_clusters_file' : join(new_sorting_directory, 'spike_clusters.npy'),
                'mean_waveforms_path' : join(new_sorting_directory, 'mean_waveforms.npy'),
                'channels' : channels,
                'units' : units,
                #'lfp' : lfp_dict
            }

        probes.append(probe_dict)


    if module == 'allensdk.brain_observatory.ecephys.align_timestamps':

        dictionary = \
        {
           'sync_h5_path' : glob(join(directory, '*.sync'))[0],
           "probes" : probes,
        }

    elif module == 'allensdk.brain_observatory.ecephys.stimulus_table':

        dictionary = \
        {
           'stimulus_pkl_path' : glob(join(directory, '*.stim.pkl'))[0],
           'sync_h5_path' : glob(join(directory, '*.sync'))[0],
           'output_stimulus_table_path' : os.path.join(directory, 'stim_table_allensdk.csv'),
           'output_frame_times_path' : os.path.join(directory, 'frame_times.npy'),

           "log_level" : 'INFO'
        }

    elif module == 'allensdk.brain_observatory.extract_running_speed':

        dictionary = \
        {
           'stimulus_pkl_path' : glob(join(directory, '*.stim.pkl'))[0],
           'sync_h5_path' : glob(join(directory, '*.sync'))[0],

           'output_path' : join(directory, 'running_speed.h5'),

           "log_level" : 'INFO'
        }

    elif module == 'allensdk.brain_observatory.ecephys.optotagging_table':

        dictionary = \
        {
            'opto_pickle_path' : glob(join(directory, '*.opto.pkl.v2'))[0],
            'sync_h5_path' : glob(join(directory, '*.sync'))[0],
            'output_opto_table_path' : join(directory, 'optotagging_table.csv')
        }

    elif module == 'allensdk.brain_observatory.ecephys.write_nwb':

        session_string = os.path.basename(probe_directories[0])
        YYYY = int(session_string[17:21])
        MM = int(session_string[21:23])
        DD = int(session_string[23:25])

        dictionary = \
        {
           "log_level" : 'INFO',
           "output_path" : nwb_output_path,
           "session_id" : int(LIMS_session_id),
           "session_start_time" : datetime.datetime(YYYY, MM, DD, 0, 0, 0).isoformat(),
           "stimulus_table_path" : os.path.join(directory, 'stim_table_allensdk.csv'),
           "probes" : probes,
           "running_speed_path" : join(directory, 'running_speed.h5')#,
          # "optotagging_table_path" : join(directory, 'optotagging_table.csv')
        }


    with io.open(output_file, 'w', encoding='utf-8') as f:
            f.write(json.dumps(dictionary, ensure_ascii=False, sort_keys=True, indent=4))

    return dictionary, last_unit_id



def cleanUpNanAndInf(value):

    if np.isnan(value) or np.isinf(value):
        return -1
    else:
        return value